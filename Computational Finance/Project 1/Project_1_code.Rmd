---
title: "Project 1"
output: pdf_document
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F, fig.width = 6, fig.height = 4, fig.align = "center")

```

### Niko Hiananto
## Question 1. Random Number Generators

```{r}
##LGM
seed <- as.numeric(Sys.time())*100

lgm_unif <- function(n){
  m = 2^31 - 1
  a = 7^5
  
  #initialize time
  x0 = seed
  
  xn = numeric(n+1)
  xn[1] = x0
  #loop
  for(i in 2:length(xn)){
    xn[i] = (a * xn[i-1]) %% m
  }
  
  seed <<- tail(xn,1)
  
  unif = (xn+0.5) /m
  
  return(unif[-1])
}

lgm_unif_seq = lgm_unif(100000)
```
The empirical mean of the generated uniform random numbers is
```{r}
mean(lgm_unif_seq)
```
which is close to the theoretical value of 0.5 and the empirical standard deviation of the generated uniform random numbers is
```{r}
sd(lgm_unif_seq)
```
which is also pretty close to the theoretical value of $\sqrt{1/12}$.

We will now compare the uniform sequence generated through LGM and the uniform sequence generated by the built-in functions in R $runif$.
```{r}
hist(lgm_unif_seq, main = 'LGM Uniform', xlab = 'Value')
unif_seq_base = runif(100000)
hist(unif_seq_base, main = 'Built-In Uniform', xlab = 'Value')
```

The uniform random numbers generated by the LGM method and by the built-in function in R are quite similar as seen by the two histograms plotted above.

\newpage
## Question 2. Discrete Probabilities

```{r}
##2. General Discrete Distributions
#create function
general_discrete <- function(n){
  #probabilities
  p = c(0.3,0.35,0.2,0.15)
  #value of probabilities
  x = c(-1,0,1,2)
  
  #generate n uniform numbers
  unif = lgm_unif(n)
  
  #create a list of indexes where the uniform number is less than the cumulative sum of the probabilities
  index = unlist(Map(function(x) min(which(x < cumsum(p))), unif))
  
  #return the values of the indexes
  return(x[index])
}

discrete_n = general_discrete(10000)
```

The histogram for the 10,000 generated sequence of discrete probabilities is shown below
```{r}
hist(general_discrete(100000), main = 'General Discrete Distribution', xlab = 'Values')
```

The mean of the general discrete sequence of 10,000 numbers is
```{r}
mean(discrete_n)
```
and the empirical standard deviation of the sequence is
```{r}
sd(discrete_n)
```

\newpage
## Question 3. Binomial Distribution

```{r}
###3. Binomial
#create bernoulli
bernoulli <- function(n, p){
  return(ifelse(lgm_unif(n) <= p, 1, 0))
}

#binomial distribution is a sum of bernoulli's
binomial <- function(out, n, p){
  #create multiple bernoulli's
  num = bernoulli(out * n, p)
  #sum the bernoulli's to get binomial distribution
  result = unname(tapply(num, (seq_along(num) - 1) %/% n, sum))
  return(result)
}
```

The histogram of the 1000 binomial distribution is shown below

```{r}
binom = binomial(1000, 44, 0.64)
hist(binom, xlab = "Value", main = "Binomial (44, 0.64)")
```

The exact probability P(X >= 40) of a Binom(44, 0.64) is
```{r}
#exact value for CDF binomial
cdf_binom <- function(k, n, p){
  i = 0:k
  res = sum(choose(n, i) * p^i * (1-p)^(n - i))
  return(res)
}

#P(X >= 40) = 1 - P(X < 40)
1 - cdf_binom(39, 44, 0.64)
```

and the empirical probability using 1,000 samples of binomial distribution is
```{r}
sum(binomial(1000, 44,0.64) >= 40)/1000
```
The actual probability is very close to 0, the empirical probability found using 1,000 samples is 0 because the sample of the binomial distribution is too small. If we try again with 1,000,000 samples we get that the empirical probability is
```{r}
sum(binomial(1000000, 44,0.64) >= 40)/1000000
```
which is close to the theoretical value.

\newpage
## Question 4. Exponential Distribution

```{r}
### Exponential
exponential <- function(n, lambda){
  result = -(1/lambda) * log(lgm_unif(n))
  return(result)
}
num = 10000
exp_rand = exponential(num, 1.5)



cdf_exp <- function(lambda, x){
  return( (1 - exp(-lambda * x)) )
}
```

The empirical probability $P(X \ge 1)$ is
```{r}
sum(exp_rand >= 1) /num
```
compared to the theoretical value:
```{r}
1 - cdf_exp(1.5, 1)
```
they are close to each other

and the empirical probability $P(X \ge 4)$ is
```{r}
sum(exp_rand >= 4) /num
```
compared to the theoretical value:
```{r}
1 - cdf_exp(1.5, 4)
```
they are also quite close to each other.

The 10,000 generated exponential distributed random numbers with $\lambda = 1.5$ is shown by the histogram below:
```{r}
hist(exp_rand)
```
The empirical mean is
```{r}
mean(exp_rand)
```
and the empirical sandard deviation of the generated sequence is
```{r}
sd(exp_rand)
```

\newpage
## Question 5. Normal Distribution

```{r}
### Normal
##Box-Mueller
box_mueller <- function(n){
  out = n
  #if not even add 1 more since need 2 for each couple of normals
  n = ifelse(n %%2 == 0, n, n+1) 
  #generate n uniforms
  unif = lgm_unif(n)
  #create couples of normals for each couple of uniform
  z1 = sqrt(-2 * log(unif[1:(n/2)])) * cos(2 * pi *  unif[( (n/2)+1 ): length(unif)] )
  z2 = sqrt(-2 * log(unif[1:(n/2)])) * sin(2 * pi *  unif[( (n/2)+1 ): length(unif)] )
  #return only n number of normals
  return( c(z1,z2)[1:out])
}

##Polar Marsaglia
polar_marsaglia <- function(n){
  #create placeholder vector to store
  out = n
  result = NULL
  n = ifelse(n %%2 == 0, n, n+1)
  
  while(length(result) < out){
    u = lgm_unif(2)
    v = 2*u - 1
    w = v[1]^2 + v[2]^2
    
    if(w <= 1){
      z = v * sqrt(-2 * log(w) / w)
      result = append(result, z)
    } 
  }
  return(result[1:out])
}


ptm <- proc.time()
gen_box = box_mueller(5000)
box_time = proc.time() - ptm

ptm <- proc.time()
gen_polar = polar_marsaglia(5000)
polar_time = proc.time() - ptm
```

The histogram of the generated 5,000 normals using Box-Mueller is shown below
```{r}
hist(gen_box, main = "5,000 Box-Mueller N(0,1) Samples")
```
and the one generated using Polar-Marsaglia is shown below
```{r}
hist(gen_polar, main = "5,000 Polar-Marsaglia N(0,1) Samples")
```

The time required to generate 5,000 normals using Box-Mueller is
```{r}
box_time["elapsed"]
```
whereas using the Polar-Marsaglia method is
```{r}
polar_time["elapsed"]
```
From the results above we can see that the Box-Mueller method can generate sequences faster than the Polar-Marsaglia method. Hence, the time taken to evaluate sin and cos is less than to evaluate the log function and check the unit circle (Polar-Marsaglia).

